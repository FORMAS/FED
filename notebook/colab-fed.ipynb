{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1UjQX2TzMTp"
      },
      "source": [
        "FED - Frame Event Detection\n",
        "\n",
        "[![Github](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/FORMAS/FED)\n",
        "\n",
        "[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://hub.docker.com/r/andersonsacramento/fed)\n",
        "\n",
        "\n",
        "\n",
        "# DESCRIPTION\n",
        "FED is a closed domain event detector system for sentences in the Portuguese language. It detect events from sentences, i.e., event trigger identification and classification. The event types are based on the typology of the FrameNet project (BAKER, 2017). The models were trained on an enriched TimeBankPT (COSTA; BRANCO,2012) corpus.\n",
        "\n",
        "\n",
        "Currently, in this Colab, 5 different trained models are available to execution: 0, 5, 25, 50, and 100 which respectively correspond to: 214, 137, 31, 13, and 5 event types.\n",
        "\n",
        "## How to cite this work\n",
        "\n",
        "Peer-reviewed accepted paper:\n",
        "\n",
        "10th Brazilian Conference on Intelligent Systems (BRACIS)\n",
        "\n",
        "* Sacramento A. ; Souza M. . Joint Event Extraction with Contextualized Word Embeddings for the Portuguese \n",
        "Language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygWWitrx0K8H"
      },
      "source": [
        "# Download and locate BERTimbau Base model and FED model files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUl60KWkzKHg",
        "outputId": "9a854386-b331-4fde-d9a7-1ad5d950bd88"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNZKTKg20Q16",
        "outputId": "09af51ba-34ec-42c8-cd5c-0a02e305ad4e"
      },
      "source": [
        "!gdown --id 13d7PKSp6dRLeMeThraA6tZ_oSUm-DRCu --output fed.zip\n",
        "!unzip fed.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13d7PKSp6dRLeMeThraA6tZ_oSUm-DRCu\n",
            "To: /content/fed.zip\n",
            "38.2MB [00:01, 27.8MB/s]\n",
            "Archive:  fed.zip\n",
            "   creating: res/\n",
            "  inflating: res/events_by_pos_types_137.json  \n",
            "  inflating: res/events_by_pos_types_31.json  \n",
            "  inflating: res/events_by_pos_types_5.json  \n",
            "  inflating: res/events_by_pos_types_13.json  \n",
            "  inflating: res/events_by_pos_types_214.json  \n",
            "   creating: models/\n",
            "  inflating: models/edff_0.h5        \n",
            "  inflating: models/edff_5.h5        \n",
            "  inflating: models/edc1ff_50.h5     \n",
            "  inflating: models/edc1ff_100.h5    \n",
            "  inflating: models/edff_25.h5       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey3-kZpQ0Twf",
        "outputId": "bf37451a-56c9-486a-ecf1-970bb38b3f52"
      },
      "source": [
        "!gdown --id 1qIR2GKpBqB-sOmX0Q5j1EQ6NSugYMCsX --output bertimbau.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qIR2GKpBqB-sOmX0Q5j1EQ6NSugYMCsX\n",
            "To: /content/bertimbau.zip\n",
            "1.21GB [00:08, 138MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LgDZAY70W1f",
        "outputId": "76fceaf1-79b7-426f-f59b-29d1c65cfcb2"
      },
      "source": [
        "!mv bertimbau.zip models/\n",
        "!unzip models/bertimbau.zip -d models/\n",
        "!rm models/bertimbau.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  models/bertimbau.zip\n",
            "  inflating: models/BERTimbau/bert_model.ckpt.index  \n",
            "  inflating: models/BERTimbau/bert_config.json  \n",
            "  inflating: models/BERTimbau/vocab.txt  \n",
            "  inflating: models/BERTimbau/bert_model.ckpt.meta  \n",
            "  inflating: models/BERTimbau/bert_model.ckpt.data-00000-of-00001  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXiiDF6M0b3U"
      },
      "source": [
        "# Load FED code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvp0CoYY0aXa",
        "outputId": "7341b2e5-56ff-4ce3-ef46-2dccdfbde457"
      },
      "source": [
        "!pip install tensorflow>=2.6.0\n",
        "!pip install keras-bert>=0.88\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsMu4hqK0kkZ"
      },
      "source": [
        "## load functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFK5NDGW0ina"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "BERTIMBAU_MODEL_PATH = 'models/BERTimbau/'\n",
        "EMBEDDING_ID = 'sum_all_12'\n",
        "\n",
        "\n",
        "RUN_CONFIGS = {\n",
        "    '100': {'model':'models/edc1ff_100.h5',\n",
        "            'events-types': 'res/events_by_pos_types_5.json'},\n",
        "    '50':  {'model':'models/edc1ff_50.h5',\n",
        "            'events-types': 'res/events_by_pos_types_13.json'},\n",
        "    '25':  {'model':'models/edff_25.h5',\n",
        "            'events-types': 'res/events_by_pos_types_31.json'},\n",
        "    '5':   {'model':'models/edff_5.h5',\n",
        "            'events-types': 'res/events_by_pos_types_137.json'},\n",
        "    '0':   {'model':'models/edff_0.h5',\n",
        "            'events-types': 'res/events_by_pos_types_214.json'}}\n",
        "\n",
        "DEFAULT_RUN_CONFIG = '0'\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_and_compose(text):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        text_tokens = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            split_token = token.split(\"##\")\n",
        "            if len(split_token) > 1:\n",
        "                token = split_token[1]\n",
        "                text_tokens[-1] += token\n",
        "            else:\n",
        "                text_tokens.append(token)\n",
        "        if len(text_tokens[1:-1]) == 1:\n",
        "          return text_tokens[1]\n",
        "        else:\n",
        "          return text_tokens[1:-1]\n",
        "\n",
        "\n",
        "def compose_token_embeddings(sentence, tokenized_text, embeddings):\n",
        "        tokens_indices_composed = [0] * len(tokenized_text)\n",
        "        j = -1\n",
        "        for i, x in enumerate(tokenized_text):\n",
        "            if x.find('##') == -1:\n",
        "                j += 1\n",
        "            tokens_indices_composed[i] = j\n",
        "        word_embeddings = [0] * len(set(tokens_indices_composed))\n",
        "        j = 0\n",
        "        for i, embedding in enumerate(embeddings):\n",
        "            if j == tokens_indices_composed[i]:\n",
        "                word_embeddings[j] = embedding\n",
        "                j += 1\n",
        "            else:\n",
        "                word_embeddings[j - 1] += embedding\n",
        "        return word_embeddings\n",
        "\n",
        "    \n",
        "\n",
        "def extract(text, options={'sum_all_12':True}, seq_len=512, output_layer_num=12):\n",
        "        features = {k:v for (k,v) in options.items() if v}\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        indices, segments = tokenizer.encode(first = text, max_len = seq_len)\n",
        "        predicts = model_bert.predict([np.array([indices]), np.array([segments])])[0]\n",
        "        predicts = predicts[1:len(tokens)-1,:].reshape((len(tokens)-2, output_layer_num, 768))\n",
        "\n",
        "        for (k,v) in features.items():\n",
        "            if k == 'sum_all_12':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts.sum(axis=1))\n",
        "            if k == 'sum_last_4':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-4:,:].sum(axis=1))\n",
        "            if k == 'concat_last_4':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-4:,:].reshape((len(tokens)-2,768*4)))\n",
        "            if k == 'last_hidden':\n",
        "                features[k] = compose_token_embeddings(text, tokens[1:-1], predicts[:,-1:,:].reshape((len(tokens)-2, 768)))\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence_original_tokens(sentence, tokens):\n",
        "        token_index = 0\n",
        "        started = False\n",
        "        sentence_pos_tokens = []\n",
        "        i = 0\n",
        "        while i < len(sentence):\n",
        "                if sentence[i] != ' ' and not started:\n",
        "                        start = i\n",
        "                        started = True\n",
        "                if sentence[i] == tokens[token_index] and started:\n",
        "                        sentence_pos_tokens.append(sentence[i])\n",
        "                        started = False\n",
        "                        token_index += 1\n",
        "                elif i<len(sentence) and (sentence[i] == ' ' or tokenize_and_compose(sentence[start:i+1]) == tokens[token_index] ) and started:\n",
        "                        sentence_pos_tokens.append(sentence[start:i+1])\n",
        "                        start = i+1\n",
        "                        started = False\n",
        "                        token_index += 1\n",
        "                i += 1\n",
        "        return sentence_pos_tokens\n",
        "\n",
        "\n",
        "def get_text_location(text, arg, start_search_at=0):\n",
        "        text = text.lower()\n",
        "        arg = arg.lower()\n",
        "        pattern = re.compile(r'\\b%s\\b' % arg)\n",
        "        match = pattern.search(text, start_search_at)\n",
        "        if match:\n",
        "                return (match.start(), match.end())\n",
        "        else:\n",
        "                return (-1, -1)\n",
        "\n",
        "\n",
        "            \n",
        "def load_bertimbau_model():    \n",
        "        global tokenizer\n",
        "        global model_bert\n",
        "        \n",
        "        paths = get_checkpoint_paths(BERTIMBAU_MODEL_PATH)\n",
        "\n",
        "        model_bert = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=512, output_layer_num=12)\n",
        "\n",
        "        token_dict = load_vocabulary(paths.vocab)\n",
        "        tokenizer = Tokenizer(token_dict)\n",
        "\n",
        "def load_fed_model():\n",
        "        global model\n",
        "        global events_types\n",
        "\n",
        "        events_types  = load_events_info()\n",
        "        model = load_model(RUN_CONFIGS[model_config]['model'])\n",
        "        return model\n",
        "\n",
        "def load_events_info():\n",
        "        events_types = {}\n",
        "\n",
        "        with open(RUN_CONFIGS[model_config]['events-types'], 'r') as read_content:        \n",
        "                events_types = json.load(read_content)\n",
        "                \n",
        "        return events_types\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def detect_events_c1ff(text, feature_option):\n",
        "    MAX_SEQUENCE_LENGTH = 150\n",
        "\n",
        "    text_tokens = get_sentence_original_tokens(text, tokenize_and_compose(text))\n",
        "    features = extract(text, {feature_option:True})[feature_option]\n",
        "    x_pred = np.zeros((1, MAX_SEQUENCE_LENGTH, 768))\n",
        "    embedding = np.array(features).reshape((len(text_tokens), 768))\n",
        "    x_pred[0,:embedding.shape[0]] = embedding\n",
        "    prediction = model.predict(x_pred)\n",
        "    positions = list(filter((lambda i: i>= 0 and i < len(text_tokens)), [pos if np.argmax(pred_value) > 0 else -1 for (pos, pred_value) in enumerate(prediction[0])]))\n",
        "    output = []\n",
        "    if len(positions) > 0:\n",
        "        start_at = sum([len(token) for token in text_tokens[:positions[0]]])\n",
        "    for pos in positions:\n",
        "        loc_start, loc_end = get_text_location(text, text_tokens[pos], start_at)\n",
        "        start_at = loc_end\n",
        "        event_type = events_types[str(np.argmax(prediction[0][pos]))]\n",
        "        output.append({'text':  text[loc_start:loc_end],\n",
        "                       'start': loc_start,\n",
        "                       'end':   loc_end,\n",
        "                       'event_type': event_type['name'] })\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def detect_events_ff(text, feature_option):\n",
        "    text_tokens = get_sentence_original_tokens(text, tokenize_and_compose(text))\n",
        "    features = extract(text, {feature_option:True})[feature_option]\n",
        "    embedding = np.array(features).reshape((len(text_tokens), 768))\n",
        "    prediction = [model.predict(e.reshape((1, 768))) for e in embedding ]\n",
        "    positions = list(filter((lambda i: i>= 0 and i < len(text_tokens)), [pos if np.argmax(pred_value) > 0 else -1 for (pos, pred_value) in enumerate(prediction)]))\n",
        "\n",
        "    output = []\n",
        "    if len(positions) > 0:\n",
        "        start_at = sum([len(token) for token in text_tokens[:positions[0]]])\n",
        "    for pos in positions:\n",
        "        loc_start, loc_end = get_text_location(text, text_tokens[pos], start_at)\n",
        "        start_at = loc_end\n",
        "        event_type = events_types[str(np.argmax(prediction[pos]))]\n",
        "        output.append({'text':  text[loc_start:loc_end],\n",
        "                       'start': loc_start,\n",
        "                       'end':   loc_end,\n",
        "                       'event_type': event_type['name'] })\n",
        "    return output\n",
        "\n",
        "\n",
        "def detect_events(text, feature_option):\n",
        "    if model_config in ['100', '50']:\n",
        "        return detect_events_c1ff(text, feature_option)\n",
        "    else:\n",
        "        return detect_events_ff(text, feature_option)\n",
        "    \n",
        "    \n",
        "def detect_from_files(input_path, output_path):\n",
        "        for filepathname in glob.glob(f'{input_path}*.txt'):\n",
        "                extractions = []\n",
        "                for line in open(filepathname):\n",
        "                        line = line.strip()\n",
        "                        print(line)\n",
        "                        extractions.append(detect_events(line, EMBEDDING_ID))\n",
        "\n",
        "                filename = filepathname.split('.txt')[0].split(os.sep)[-1]\n",
        "                with open(f'{output_path}{filename}.json', 'w')  as outfile:\n",
        "                        json.dump(extractions, outfile)\n",
        "                print(f'{filename}')\n",
        "\n",
        "\n",
        "def detect_events_from(input_path, output_path):\n",
        "        run_detect_context(lambda : detect_from_files(input_path, output_path))\n",
        "        \n",
        "\n",
        "def detect_events_from_sentence(sentence):\n",
        "        sentence = sentence.strip()\n",
        "        run_detect_context(lambda : print(detect_events(sentence, EMBEDDING_ID)))\n",
        "        \n",
        "\n",
        "def run_detect_context(run_detect_func):                        \n",
        "        if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "                with tf.device('/GPU:0'):\n",
        "                        load_bertimbau_model()\n",
        "                        load_fed_model()\n",
        "                        run_detect_func()\n",
        "        else:\n",
        "                with tf.device('/cpu:0'):\n",
        "                        load_bertimbau_model()\n",
        "                        load_fed_model()\n",
        "                        run_detect_func()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAxs_K0z04cP"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGa4ZOM07v1"
      },
      "source": [
        "## Detect Events From Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nUaXPW00mt"
      },
      "source": [
        "#@title Input the sentence and select the model\n",
        "\n",
        "sentence = 'A Petrobras aumentou o pre√ßo da gasolina para 2,30 reais, disse o presidente.' #@param {type:\"string\"}\n",
        "model_config = '0' #@param [\"0\", \"5\", \"25\", \"50\", \"100\"]\n",
        "\n",
        "\n",
        "print(sentence)\n",
        "print(model_config)\n",
        "detect_events_from_sentence(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xgNRe51Puz"
      },
      "source": [
        "## Extract Events From Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_oWmwIb1LaB",
        "outputId": "c6014d34-9a65-497b-c742-3bc86c250c8e"
      },
      "source": [
        "# If you want to be able to process files from your drive folders \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsgISvVS1WiZ"
      },
      "source": [
        "#@title ## Input and Output directory fields\n",
        "\n",
        "#@markdown The text files in the input directory are expected to have the format:\n",
        "\n",
        "#@markdown * all text files end with the extension .txt\n",
        "#@markdown * sentences are separated by newlines\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter the directories paths:\n",
        "input_dir = \"/content/drive/MyDrive/input-files/\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/drive/MyDrive/output-files/\" #@param {type:\"string\"}\n",
        "model_config = '0' #@param [\"0\", \"5\", \"25\", \"50\", \"100\"]\n",
        "#@markdown ---\n",
        "\n",
        "detect_events_from(input_dir, output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}